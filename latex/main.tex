\input{Packages}
\begin{document}
    \nopagecolor
\begin{center}
        \text{}
	\vspace{5mm}
	\textbf{\Large Simulating the Best Waveforms for Any Shuttling Ramps\\ \normalsize A Tool for Ion Shuttling Methodologies}
	
	\vspace{19mm}
	{\normalsize Oneka Singh \\}
	\vspace{5mm}
	
	{\normalsize 03.08.2025\\}
	\vspace{10mm}
\end{center}

\newpage

\begin{flushleft}
	\tableofcontents
\end{flushleft}

\newpage

\section{Overview}
\subsection{Introduction and Goal} 

In an ion trap, ions can be shuttled in many ways, each corresponding to a specific voltage ramp. Some of these ways include: swap, merge, split, transport, etc.
\\
This paper focuses on the transport voltage waveform for the experimental red trap setup. 
\\
When waveforms are passed through the electrodes in the ion trap, there can be noise effects induced by the applied voltages. To combat noise, which is a large issue due to the sensitivity of the ion's state, low-pass FIR filters are applied. In turn, the input voltage waveform becomes distorted in some way. More specifically, for the experiment's filters, the waveform becomes convoluted (see terminology).
\\
To restore the correct waveform while filtering electrical noise, a filter precompensation algorithm is employed. It passes an alternative input waveform, which when filtered, yields the original waveform to a degree of accuracy. In other words, the algorithm finds an input waveform that outputs the original waveform before filtering, essentially "canceling" the convoluted output.
\\
The goal of this paper is to develop an understanding of how the experimental filters can be simulated, convoluted, and precompensated, all while optimizing to minimize the error between the estimation waveform and the original waveform. The best transport waveforms have minimal error while being as small in time as possible. In addition to precompensation in general, precompensating optimally (small and accurate) is the end objective of the paper, in addition to seeing the tradeoff and limits of the precompensation technique on the passed waveforms.

\subsection{Algorithm Steps}
The filter simulation contains the following main steps:

\begin{enumerate}
    \item Setup and structure input data: waveform and time arrays, minimum DAQ step function
    \item Import the filter kernel as an impulse response
    \item Simulate the effect of the desired filter as a convolution on the input waveform
    \item Precompensate the input waveform to estimate the desired output result (this is the heart of the algorithm)
    \item Optimize parameters: less padding, minimal error, maximum compression, best regularization strength
    \item Bound the optimization within the limits of the filter's maximum and minimum voltage range
\end{enumerate}

\subsection{Terminology}
Factors that are optimized (which are discussed more in the Optimize section) include the compression, padding, regularization, and error. These values come up throughout the paper, and what is meant by them is explained below.
\begin{definition}
     \textbf{Compression:} The compression factor, which is 1 for uncompressed waveforms, is a multiplicative value that scales the waveform on the time axis. As part of the goal to minimize the time of the waveform, a larger compression value corresponds to temporally smaller waveforms:
    \begin{center}
        Compression $> 1 \rightarrow$ Larger Waveform than Original \\
        Compression $= 1 \rightarrow$ Equal Waveform to Original \\
        Compression $> 1 \rightarrow$ Smaller Waveform than Original
    \end{center}
    Codewise, the compression is given by dividing the input time array of the waveform (total time) by the compression factor:
    \begin{minted}{python}
        time_array / best_compression
    \end{minted}
    For a time array that spans 0 to 19.38 $\mu$s, for example, a compression of 20x would be:
    \begin{center}
        $\dfrac{19.38 \mu s}{20} = 0.969 \mu s$
    \end{center}
    This means the new temporal length of the waveform would be 0.969$\mu$s.
\end{definition}

\begin{definition}
     \textbf{Padding:} The padding is applied on both edges of the waveform, extending its entire length. This is done because the precompensated waveform has edge ringing values. These values arise from there being nothing to precompensate on the ends of the function, and thus need to be suppressed. 
    \begin{center}
        Larger Padding $\rightarrow$ Larger Waveform \\
        Smaller Padding $\rightarrow$ Smaller Waveform
    \end{center}
\end{definition}
Since the goal is to reduce the length of the waveform, the padding should be reduced as much as possible to account for just the edge ringing. In the code, the padding is added to the original waveform at the start, and the ringing edges of the precompensated waveform are then suppressed for the padded values.

\begin{definition}
     \textbf{Error:} The error between the original waveform and the precompensated filtered (estimated original) is calculated over the entire waveform, on a case-by-case basis (not taken over multiple runs). The goal is to minimize the error between the two waveforms, making the estimation as close as possible to the original. There are two kinds of errors that are tracked. 
     
     The first and most important is the maximum absolute error, which corresponds to the largest error point at a certain time for the entire waveform. Graphically, this point is where the two estimated and original waveforms overlap, so minimizing the maximum absolute error means a closer estimation. This error is minimized in the optimization step to get higher accuracy estimation waveforms.
\\
     The second error briefly mentioned in plots and printed outputs is the rmse, which is the root mean squared error of the complete function. This is the root of the mean squared error over the entire function. This value generally decreases when the maximum absolute error decreases.
\end{definition}

\begin{definition}
    \textbf{Regularization:} The regularization term is explained in detail during the precompensation step of the filter algorithm. Simply, however, it ensures that there is no division by zero during the precompensation mathematics. For different waveform sizes and shapes, the best regularization value extends from $1e^{-1}$ to $ke^{-n}$, and varies on a case-by-case basis. In the algorithm, the best regularization in dependence on the padding and compression values is found through optimization, which is mentioned later.
\end{definition}

\section{The Algorithm}
\subsection{Filter Simulation Class}
The filter simulation class is responsible for filtering and precompensating the input waveforms. This starts with setting up the input time and waveform arrays, and getting the input filter kernel. With the input data in place, the waveform can then be run through the filter, and interpolated to match the length of the time array. At the heart of the algorithm is the precompensation step, which estimates the correct precompensated waveform based on the effect of the filtering. The edge ringing of the precompensated waveform is suppressed by setting these edge values to a constant value, and then the final waveform is plotted.
\subsubsection{Importing Libraries}

The first step is to import the necessary libraries for the FilterSimulation class.

\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.fftpack import fft, ifft
from scipy.interpolate import interp1d
import seaborn as sns
from sklearn.metrics import mean_squared_error
import matplotlib.patches as patches
import matplotlib.colors as mcolors 
\end{minted}

Numpy is used very often for constructing arrays, operations, and mathematics. Matplotlib is used to plot results. Scipy applies advanced mathematical operations such as the Fourier transform and interpolation on the precompensated waveform (this is explained in the precompensation section). Seaborn and sklearn are used in plotting and as metrics with which the waveform will be optimized (more on this in the Optimize class section).

\subsubsection{Filter Method}

After importing, the FilterSimulation() class is defined. The first method mentioned sets up the filter kernel through which the waveforms will be passed. The given filter kernel corresponds to a low-pass FIR filter used experimentally in the red trap quantum computer. 
\\
The input data type for the given filter kernel is an array of its coefficients. The associated filter kernel weights for the red trap's filter are given by the graph below:

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/kernel.png}
    \caption{Kernel weights for the FIR filter. These weights correspond to the filtering behavior on the waveforms passed through the electrodes in the red trap.}
\end{figure} \label{kernel}

In the filter() method, the weights are first loaded from an npy file as a step response:

\begin{minted}{python}
    def filter(self):
        step_response = np.load('filter_data.npy')
\end{minted}

Then, the impulse response is defined and returned. The impulse response describes how the step response reacts to the input waveform. It is the way filtering affects the later detailed precompensation function.

\begin{minted}{python}
    impulse_response = np.diff(step_response, prepend = 0)
    return impulse_response
\end{minted}

\subsubsection{The Setup}
With the filter kernel defined, the time scale and waveform arrays can be input. It is important to note here that there are some important initial parameters to clarify. 
\\
The time step value, which corresponds to the DAQ's minimum sampling time between varying voltage points, is a lower bound restriction on the resolution of the signal. Smaller time steps allow more data points in the given waveform range, leading to higher accuracy. It is important to know the limitations of the hardware in the simulation. In the red trap's hardware, the DAQ has a sampling rate corresponding to 380 ns, which sets the time step value in this code.
\\
Additionally, the step size of the waveform (height in volts) is another input factor. The given waveform in this code has a step size of 6 volts, which corresponds to how much the voltage varies over transport.

Both of these parameters are given below:

\begin{minted}{python}
# input values:
waveform_size = -6 # volts
time_step = 0.38 # DAQ lower limit
\end{minted}

Next comes the input time array and waveform data. Both of these values can range in length, but must match in order for the plotting to align. In the given case, both of these values have a length of 51.

\begin{minted}{python}
time_array = np.array([0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18,
0.2, 0.22, 0.24, 0.26, 0.28, 0.3, 0.32, 0.34, 0.36, 0.38, 0.4, 0.42, 0.44, 0.46, 
0.48, 0.5, 0.52, 0.54, 0.56, 0.58, 0.6, 0.62, 0.64, 0.66, 0.68, 0.7, 0.72, 0.74, 
0.76, 0.78, 0.8, 0.82, 0.84, 0.86, 0.88, 0.9, 0.92, 0.94, 0.96, 0.98, 1])

waveform = np.array([-1, -0.9990133642, -0.9960573507, -0.9911436254, 
-0.9842915806, -0.9755282581, -0.9648882429, -0.9524135262, -0.93815334, 
-0.9221639628, -0.9045084972, -0.8852566214, -0.8644843137, -0.842273553, 
-0.8187119949, -0.7938926261, -0.7679133975, -0.7408768371, -0.7128896458, 
-0.6840622763, -0.6545084972, -0.6243449436, -0.5936906573, -0.5626666168, 
-0.5313952598, -0.5, -0.4686047402, -0.4373333832, -0.4063093427, 
-0.3756550564, -0.3454915028, -0.3159377237, -0.2871103542, -0.2591231629, 
-0.2320866025, -0.2061073739, -0.1812880051, -0.157726447, -0.1355156863, 
-0.1147433786, -0.0954915028, -0.0778360372, -0.06184666, -0.0475864738, 
-0.0351117571, -0.0244717419, -0.0157084194, -0.0088563746, -0.0039426493, 
-0.0009866358, 0]) + 1
\end{minted}

A '+1' factor is added to the end of the waveform to ensure that the signal begins at zero--otherwise, the precompensation algorithm strays from the input waveform.
\\
The time array is normalized to a length of one. The total length of the filter can be calculated by multiplying the waveform length by that of the time step:

\begin{center}
    $51 \times 380ns = 51 \times 0.38 \mu s = 19.38 \mu s$
\end{center}

In code, this is:

\begin{minted}{python}
total_time = len(waveform) * time_step # 19.38 microseconds, NOT 17.5 microseconds
\end{minted}

Then the number of samples is the length of the waveform, which is 51:

\begin{minted}{python}
num_samples = int(total_time / time_step) # len(waveform)
\end{minted}

The time array and waveform can now be scaled up to the desired size:

\begin{minted}{python}
time_array *= total_time
waveform *= waveform_size
\end{minted}
Mathematically, this is:
\begin{center}
    time array: $(0, 1) \rightarrow (0, (1 \times 19.38 )) \rightarrow (0, 19.38)$ in $\mu$s
    \\
    waveform: $(0, 1) \rightarrow (0, (1 \times 6)) \rightarrow (0, 6)$ in volts
\end{center}

This gives a time scale of 0 to 19.38 $\mu$s and a waveform with 51 points, all spaced at distances of 380 ns.
\\
Next, these values are input to the optimizer to find the best padding size, compression factor, and regularization value that yield the lowest error. This is explained more in the Optimize section later on. For now, the best parameters are taken to complete the setup step.
\\
To structure the input waveform and time array properly to allow for accurate filtering and precompensation, the setup method in the Filter Simulation class is passed:

\begin{minted}{python}
original, time, dt, first_index, last_index = FilterSimulation().setup(time_array 
/ best_compression, waveform, best_compression, num_samples, best_padding)
\end{minted}

This outputs the original waveform, the time, which is an extension of the time array that matches the original waveform, the rate of change, which is dt, and the first and last indices of the original waveform.  The next step is to look at the contents of the setup and how it retrieves these values.

\begin{minted}{python}
def setup(self, time_array, waveform, compression, num_samples, padding):
\end{minted}

The setup method takes in the time array (which can be compressed depending on the compression factor), the waveform, the number of samples, and the padding factor. As mentioned previously, the padding and compression factors are found through optimization and are discussed in more detail later on.
\\
It starts by creating a proper time axis from 0 to the last index of the time array, with the same number of samples as the waveform has.

\begin{minted}{python}
    dense_time = np.linspace(0, time_array[-1], num_samples)
\end{minted}

Then, the waveform is interpolated over the time axis values with x-coordinates being the compressed time array and y-coordinates being the waveform array.

\begin{minted}{python}
    dense_waveform = np.interp(dense_time, time_array / compression, waveform)
\end{minted}

Next the dense waveform is set to the original waveform, which will be used in the algorithm. This is done by adjusting the edge padding of the waveform to practically extend the first and last indices by an amount that is given by the optimizer. As mentioned in the terminology section, the larger the padding, the longer the waveform. We do not want this to happen, so we optimize padding to as small a value as can just mitigate the precompensation edge ringing.

\begin{minted}{python}
    original = np.pad(dense_waveform, pad_width = best_compression, mode = 'edge')
\end{minted}

Next the time axis is sliced to the length of the original waveform if it is larger than the original. This ensures that plotting aligns axes. Then, the dt value, the difference between 2 samples or the rate of change of the time axis is found. This is our 0.38 $\mu$s from earlier. To construct the final time array, the array is arranged with the length of the original function and points spaced out at dt intervals. In our experiment, with a waveform length of 51 and dt of 0.38 $\mu$s we have a total length of 19.38 $\mu$s, which matches our result from the beginning of the section.

\begin{minted}{python}
    time_sliced = dense_time[:len(original)]
    dt = time_sliced[1] - time_sliced[0]
    time = np.arange(len(original)) * dt
\end{minted}

The first and last indices of the waveform are important for the later used trimming function, which does the actual edge ringing suppression of the precompensated waveform. These value are also calculated, and everything is then returned.

\begin{minted}{python}
    first_index = original[0]
    last_index = float(original[-1])

    return original, time, dt, first_index, last_index
\end{minted}

\subsubsection{Convolution "Filter Simulation"}

The next important step is to actually simulate the effect of the filter on the original input waveform. This is done by a method from the FilterSimulation() class called convolution(waveform). It is called with:

\begin{minted}{python}
    filtered = FilterSimulation().convolution(original)
\end{minted}

This returns the filtered waveform. Taking a look at the actual contents of the convolution shows its relation to the specific FIR filter coefficients from the Filter Kernel section.

\begin{minted}{python}
    def convolution(self, waveform):
        impulse_response = self.filter()
        filtered_waveform = np.convolve(waveform, impulse_response, mode = 'full')
        return filtered_waveform
\end{minted}

First, the impulse response is defined, which is retrieved from the filter method. As detailed previously, this method sets up the filter kernel depending on the input coefficients. The impulse response shapes how the convolution works, and is based upon the filter kernel. Then, numpy's convolve() method is used on the waveform with the given impulse response. The mode 'full' ensures that the convoluted waveform's size is not warped, which would significantly distort the waveform in relation to the time axis. The output filtered waveform is then returned.

\subsubsection{Interpolation Method}

This output waveform is shifted from the time axis in some way after the convolution operation, so it is "reanchored" to the current time axis by creating a shifted time array, and interpolating over the time axis values, with the x-axis being the shift and the y-axis being the filtered waveform.

\begin{minted}{python}
    shift_f = np.arange(len(filtered)) * dt
    filtered = FilterSimulation().interpolation(time, shift_f, filtered)
\end{minted}

This calls the Filter Simulation's interpolation method. The precompensated and precompensated filtered waveforms must also be interpolated for the same reason as mentioned above (time axis shifts), so this method is created for cleanliness. 

\begin{minted}{python}
    def interpolation(self, time, shift, filtered):
        interp_func = interp1d(shift, filtered, kind='cubic', bounds_error=False, 
            fill_value="extrapolate")
        filtered_interp = interp_func(time)  # Now same length as time_original
        return filtered_interp
\end{minted}

Note that the interp1d() function from scipy is called, which is different than np.interp(). The scipy interp1d() constructs a function that estimates values between a set of known data points--in this case, between the shift and filtered arrays. The 'cubic' kind is the shape of the interpolation, and is smoother than other kinds, like 'linear'.

\subsubsection{The Filter Precompensation}

The main charm of this algorithm lies in the mathematics of the precompensation. As explained earlier, the original waveform is distorted (convoluted) when it undergoes filtering. To combat this the waveform is precompensated to take on a different input shape so that the output matches the original waveform. 
\\
The key point to precompensation and having to estimate the original waveform with a margin of accuracy arises from the fact that the function is injective. This means that it is not invertible, and you cannot restore its input based on the output. To estimate this function, a series of Fourier transformations in the frequency domain. 

The precompensation is called with:

\begin{minted}{python}
    precompensated = FilterSimulation().precompensation(original, last_index, 
        best_reg_strength, best_padding)
\end{minted}

Inside the actual precompensation method, the impulse response is first retrieved from the filter method similar to what is done earlier.

\begin{minted}{python}
def precompensation(self, waveform, last_index, reg_strength, best_padding):
    impulse_response = self.filter()
\end{minted}

Since the original waveform is now padded, the impulse response must know how to work the greater-length waveform, and the inputs are both padded to sync their length. 

\begin{minted}{python}
    total_length = len(impulse_response) + len(waveform)
    ir_padded = np.pad(impulse_response, (best_padding, total_length - 
        len(impulse_response) - best_pading), mode='constant')
    waveform_padded = np.pad(waveform, (best_padding, total_length - 
        len(waveform) - best_pading), mode='constant', constant_values=last_index)
\end{minted}

After this initial setup step begins the pure mathematics. The function is now ready to begin the precompensation estimation.
\\
Let $h(t)$ be the function of the original waveform, $g(t)$ be the effect of the filter on the input waveform, and $f(t)$ be the output convoluted waveform. Then, the relation below becomes apparent:

\begin{equation}
    f(t) = g(t) * h(t)
\end{equation}

Here, the $*$ corresponds to the \textit{convolution} operation of the filter on the input waveform. This corresponds perfectly to the convolution() method defined earlier to simulate the filter effect.
\\
Codewise, we set the padded waveform to the input $h(t)$ value:

\begin{minted}{python}
    h_t = waveform_padded
\end{minted}

Since the mathematics is being done in the frequency domain, the complex transfer function of the filter $G(\omega)$ (called G in the code) is given by: 

\begin{minted}{python}
    G = fft(ir_padded)
\end{minted}

As one can see, the complex transfer function of the filter directly relates the filter's effect to the padded impulse response, where the filter kernel comes into play. 
\\
Here, another realization arises, since $g(t)$ is the effect of the filter function, and $G(\omega)$ is the complex transfer function of the filter.

\begin{equation}
    g(t) = F^{-1}(G(\omega))
\end{equation}

This fact is useful in the grand scheme of the mathematics. If we know how we want the output waveform $f(t)$ to look (like the original signal), then the precompensated waveform to solve for is $h(t)$. For this reason, we isolate and solve for the correct precompensation waveform that yields the original waveform after convolution.

\begin{equation}
    h(t) = F^{-1}\biggr(\frac{F(\omega)}{G(\omega)}\biggr) = F^{-1} \biggr(\frac{F(f(t))}{G(\omega)}\biggr)
\end{equation}

The code then follows with taking the fourier transform of $f(t)$, which will be called $F(\omega)$:

\begin{minted}{python}
    h_w = fft(h_t)
\end{minted}

There is a big problem here. If any of the complex transfer function's values are zero, the value of $h(t)$ becomes undefined. This is because $G(\omega)$ is in the denominator. To ensure this never happens, the function undergoes regularization, which prevents division by zeros or near-zeros.

\subsubsection{Tikhonov Regularization}

In the precompensation algorithm, we invert the filter transfer function $G(\omega)$ to compute the input waveform $h(t)$ that will produce a desired output after filtering. To avoid getting close to or being zero, $G(\omega)$ is regularized. This begins by defining the complex transfer function in terms of its amplitude attenuation $A(\omega)$ and phase $\phi(\omega)$.

\begin{equation}
    G(\omega) = A(\omega)e^{i \phi(\omega)}
\end{equation}

In the code, numpy's abs() and angle() methods are used.

\begin{minted}{python}
    G_magnitude = np.abs(G) # amplitude attenuation
    G_phase = np.angle(G) # phase shift
\end{minted}

As noted above, the filter inversion becomes unstable when $G(\omega)$ is very small (at frequencies strongly attenuated by the filter). Here, dividing by zero leads to large amplification of noise or numerical artifacts (excess ringing) in the precompensated signal.
\\
Tikhonov regularization stabilizes signals at high frequencies that cause near-zero function values. This technique is demonstrated in our setup using the Tikhonov regularization equation below:

\begin{equation}
    H(\omega) = \dfrac{F(\omega) \cdot G^*(\omega)}{|G(\omega)|^2 + \lambda}
\end{equation}

The $\lambda$ value here corresponds to the regularization strength, called $reg_strength$ in the code. It is a small positive constant that stabilizes the inversion, and can be tuned to a certain value (usually in the range $e^{-5}$ to $e^{-1}$ for the best precompensation fit. This tunable parameter is optimized in the Optimizer for this reason, which is detailed in the later Optimize class section.
\\
Tikhonov regularization suppresses high-frequency components that interfere with the precompensated waveform, and provides a controlled trade-off between exact inversion and smoothness of the precompensated waveform.

In the precompensation method, this step is given by:

\begin{minted}{python}
    regularized_magnitude = G_magnitude / (G_magnitude**2 + reg_strength)
    regularized_G = regularized_magnitude * np.exp(-1j * G_phase)
\end{minted}

Here, the amplitude attenuation and phase shift become apparent. Mathematically speaking, these lines correspond to:

\begin{equation}
    \biggr( \dfrac{A(\omega)}{A(\omega)^2 + \lambda} \biggr) \cdot e^{-i \cdot \phi(\omega)}
\end{equation}

With this regularized value of $G(\omega)$, the Fourier transfomation sequence can be completed:

\begin{minted}{python}
        f_w = h_w * regularized_G
        f_t = np.real(ifft(f_w))

        return f_t
\end{minted}

Thus, the correct output $f(t)$ is found. This function represents the precompensated waveform that returns the original input waveform with the degree of accuracy determined by the regularization trade-off (larger regularization means smoother function, and smaller regularization means higher frequency spikes).

\subsubsection{Trimming Method}

Before the precompensated waveform is fully ready for use, it still has one more problem: edge ringing. As mentioned previously edge ringing on the precompensated waveform occurs because there is no waveform to precompensate on the ends of the function. This is suppressed by padding the original waveform's edges by a constant value matching that of the first and last indices. Over these extended edges, precompensated waveform's (ringing) values are suppressed. The trimming method is responsible for this suppression.
\\
First, the precompensated waveform is sliced to the length of the original to ensure matching lengths, and then the the method:

\begin{minted}{python}
    precompensated = precompensated[:len(original)]
    precompensated = FilterSimulation().trimming(precompensated, best_padding, 
        first_index, last_index)
\end{minted}

As mentioned in the setup section, this is where the first and last index values of the original waveform become important.
\\
From the beginning of the precompensated waveform (index zero) to the end of the padded segment, the precompensated array is set to a constant value of the first index. The same is done for the end, with the padded end period.

\begin{minted}{python}
def trimming(self, precompensated, best_padding, first_index, last_index):
        for i in range(0, int(best_padding*0.3)):
            precompensated[i] = first_index      
        for i in range(len(precompensated) - int(best_padding*0.5), 
            len(precompensated)):
            precompensated[i] = last_index
        return precompensated
\end{minted}

Notice that there is one explicit difference that is added to the code. This is the 'tuning' parameter in the trimming function. For the beginning padding segment, instead of going from the beginning of the signal all the way up to the end of the padding period, the suppression only goes up to the 0.3 of the way. This is because of the behavior of the precompensation waveform.
\\
In order to overcompensate accurately, the waveform begins to take shape before the start of the original waveform, meaning that it cuts into the padding zone. The 0.3 is the real 'edge' of the precompensated waveform, where the edge artifacts (ringing) become apparent.
\\
A similar reason is given to the ending pad segment, but the waveform only starts to produce ringing 0.5 the way of the padded segment--suppressing it any earlier would warp the effect of the precompensation.

\subsubsection{Filtering the Precompensated Waveform}

With a fully precompensated waveform, simulating this new input through the filter simulation provides a precompensated filtered waveform. This is the end result, and should be optimized to attain an estimation with as minimal error as possible to the original, unfiltered input waveform.
\\
This begins by running the precompensated waveform through the convolution method, just like the filtered waveform.

\begin{minted}{python}
precompensated_filtered = FilterSimulation().convolution(precompensated)
\end{minted}

As mentioned previously, the convolution can distort the waveform, which calls for interpolation to realign the waveform with the central time axis. The same process is used for the precompensated filtered waveform.
\begin{minted}{python}
shift_p = np.arange(len(precompensated_filtered)) * dt
precompensated_filtered = FilterSimulation().interpolation(time, shift_p,
precompensated_filtered)
\end{minted}

The same steps are applied to also visualize the precompensated waveform before filtering.

\begin{minted}{python}
shift_p_unfiltered = np.arange(len(precompensated)) * dt
precompensated_unfiltered = FilterSimulation().interpolation(time, 
shift_p_unfiltered, precompensated)

\end{minted}

The result is four waveforms: original, filtered, precompensated unfiltered, and precompensated filtered. These can be visualized through matplotlib plotting.

\subsubsection{Plotter Method}

The plotter method takes the four different waveforms and the cohesive time axis and plots them together. The convoluted waveform is shallower than the original and starts later, while the precompensated unfiltered waveform starts before the original. The hope is to maximize overlap between the original and precompensated filtered waveforms. 

The plotter method is first called:

\begin{minted}{python}
    FilterSimulation().plotter(original, time, filtered, precompensated_unfiltered, 
        precompensated_filtered)
\end{minted}

As seen, it takes in the 4 waveforms and the time axis. Inside the method, all four waveforms are plotted on the central time axis, and are labeled accordingly.

\begin{minted}{python}
def plotter(self, original, time, filtered, precompensated_unfiltered, 
precompensated_filtered):
    plt.figure()
    plt.plot(time, original, label = 'Original')
    plt.plot(time, filtered, label = 'Filtered')
    plt.plot(time, precompensated_unfiltered, label = 'Precompensated Unfiltered', 
        linestyle = '--')
    plt.plot(time, precompensated_filtered, label = 'Precompensated Filtered', 
        linestyle = '--')
\end{minted}

The time is still in $\mu$s, which is on the x-axis, and the y-axis is in volts.

\begin{minted}{python}
    plt.title('Filter Waveforms')
    plt.xlabel('Time in Âµs')
    plt.ylabel('Voltage')
    plt.grid(True)
    plt.legend()
    plt.show()
\end{minted}

The result of this algorithm is shown below, after running the plotter method at the end of the code. These results correspond to the aforementioned filter kernel, time array, and waveform input. The padding, and regularization strength $\lambda$ are optimized later on but are set to a constant value for simplicity in explanation here. Compression is not yet applied here.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/example1.png}
    \caption{A simple  example of applying the filter simulation algorithm on an input waveform.}
\end{figure} \label{example1}

The specific given values for this result are listed below:

\begin{center}
\begin{tabular}{ | c | c | } 
  \hline
   Input Waveform Array & [-1, -0.9990133642, ..., 0] + 1 \\ 
  \hline
  Input Time Array & [0, 0.02, ..., 1] \\ 
  \hline
  Filter Kernel Coefficients & [4.65175217e-05, ..., 1.00004649e+00] \\ 
  \hline
  Minimum DAQ Step & 380 ns \\ 
  \hline
  Total Duration & 19.38 $\mu$s \\ 
  \hline
  Waveform Size & 6 volts \\ 
  \hline
  Padding & 30 \\ 
  \hline
  Regularization Strength & 5e-05 \\ 
  \hline
  Min/Max Voltage Strength & $\pm 40$ volts \\ 
  \hline
\end{tabular}
\end{center}

Here, the calculated error is:

\begin{minted}{python}
Root Mean Squared Error (RMSE): 0.15461 V
Max Absolute Error: 0.28321 V
\end{minted}

Graphically, the maximum absolute error over time is represented as follows.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/example1error.png}
    \caption{A simple example of applying the filter simulation algorithm on an input waveform.}
\end{figure} \label{example1error}

As seen, the peak error is at 0.28321 volts, a bit above 0.25 volts. The root mean square error (rmse) takes the root of the average error, and is always lower than the max absolute error. The smaller the error margin (of rmse and max absolute error), the greater the accuracy of the estimated precompensated waveform, and the greater the overlap between the original (blue) and precompensated filtered (red) waveforms.

\subsection{Optimize Class}

The optimize simulation class optimizes the precompensated waveform to achieve:

\begin{enumerate}
    \item As low an error margin as possible $\rightarrow$ best precompensation approximation of original waveform
    \item As small a waveform as possible $\rightarrow$ faster transport sequences
\end{enumerate}

Optimizing the waveform provides a roadmap to understanding the boundaries and limitations of the precompensation approach taken in this paper. It provides a threshold given by the varying parameters that are finely tuned for filter optimization:

\begin{itemize}
    \item Minimize \textbf{padding} $\rightarrow$ smaller waveforms
    \item Maximize \textbf{compression} $\rightarrow$ smaller waveforms
    \item Minimize \textbf{max absolute error} (consequently rmse) $\rightarrow$ more accurate waveforms
    \item Find best \textbf{regularization strength} $\rightarrow$ more accurate waveforms
\end{itemize}

These parameters are further detailed in the terminology section at the beginning of this paper. The optimization class has several methods for plotting relevant data (such as the error plot from the section above), along with a main, large method that approximates the best parameters depending on an input waveform and tunable weights.

\subsubsection{Parameters and Optimization Logic}

The algorithm finds the best matching case according to the weight of three main parameters: max absolute error (alpha), padding (beta), and compression (gamma).
\\
Each parameter is given a tunable \textit{weight} according to its relevance in the result. For example, to get a waveform with minimal max absolute error, one has to increase the weight on alpha, which yields a more accurate precompensation at the stake of a larger function. Exploring the effect of different weights helps demonstrate the tradeoff between waveform size and waveform accuracy, and the true "estimation" limitation of the precompensated waveform.
\\
In code, the class is defined and initial parameters are attributes of the class, as follows:

\begin{minted}{python}
class Optimize:
        
    def __init__(self):
        self.padding_range = list(range(xx, xx))
        self.reg_exponents = list(range(-x, x))      
        self.reg_coeff = list(range(xx, xx))
        self.compression_range = list(range(xx, xx))
        
        self.lower_voltage_bound = -40
        self.upper_voltage_bound = 40

        self.alpha = xx.xx # max abs error
        self.beta = xx.xx # padding
        self.gamma = xx.xx # compression
\end{minted}

The values marked with x or xx are variable parameters that are to be tuned depending on the waveform. Usually regularization is between 9e-1 to 1e-6. Padding also ranges, and usually falls between 20 and 100 sample units. 

The weights alpha, beta, and gamma are also defined here. 
\\
The range of the padding and compression corresponds to how many values the optimizer should test. Larger ranges mean a longer runtime, so it is best to benchmark the waveform for appropriate padding and compression values beforehand, to get a general feel for its limits.
\\
The weighted parameters are summed up to a penalty value. The penalty value for each combination of parameters in the given range is stored, and the lowest penalty is chosen at the end of the loop to yield the best result.
\\
Additionally, the lower and higher voltage bounds that are restricted by the hardware of the device are also defined attributes. Together, these values are constantly used in the Optimize class, and set up the initial tweakable bounds for the behavior of the methods in the class.

\subsubsection{Optimizer Method}

The optimizer method is the heart of the optimize class. It was briefly mentioned before the setup method of the filter simulation class, as this is where it is located in the code.

\begin{minted}{python}
...
waveform *= waveform_size

best_error_grid, bounds_grid, best_compression, best_padding, best_reg_strength = 
Optimize().optimizer(waveform, time_array, num_samples) # the method of focus
    
Optimize().heatmap(best_error_grid, bounds_grid, best_padding, best_reg_strength, 
...
\end{minted}

The optimizer method takes in the initial waveform and time arrays before properly setting them up as the original waveform and time axis. This is because the parameters need to be found before the original waveform--which is padded and compressed by a certain degree--can be set up. Output are the best compression, padding, and regularization values, plus two additional values used to plot results (used by other methods in Optimize).
\\
The optimizer method is defined below, and some initial variables are defined:

\begin{minted}{python}
def optimizer(self, waveform, time_array, num_samples):
    best_penalty = float('inf')
    best_params = (None, None, None, None, None) # compression, padding, 
    reg_strength, penalty, bounds
    all_best_params = []
    reg_array = []

    compression_grids = {}
\end{minted}

Initially, the best penalty is set to infinity, so that later in the code, the first iteration will take the place of the best penalty thus far. Any penalty smaller than the initial run is then tracked, and at the end, all tracked penalties are compared to find the absolute minimum penalty case.
\\
The second defined variable is a blueprint of the five parameters to be stored for the tracked best penalty case scenarios. The five values to be stored are the compression, padding, regularization strength, calculated penalty (where the weight comes into play), and the bounds (checks voltage bounds). If the voltage of the precompensated waveform reaches above 40 volts or below -40 volts for a certain parameter combination, that result is voided. This is done by setting the bounds for all voided results to one, and all inclusive results to zero. The function later checks the bound value first, to further decrease the sample size, it needs to search for the lowest penalty.
\\
The best parameters with respect to the run are saved in a variable as well as the regularization coefficient $c$ corresponding to the current regularization exponent $n$. These values are later used to construct the full regularization strength $c \; \cdot$ e$^{-n}$.
\\
For the later mentioned heatmap method, an error grid and bounds grid are also created. The error grid finds the magnitude of the absolute max error per combination for a set of padding and regularization values. The bounds grid shows for which combinations the results are out of the hardware range of $\pm 40$ volts. Each run has a corresponding error and bound grid, two arrays which are stored in the compression grid dictionary below:

\begin{minted}{python}
    compression_grids = {}
\end{minted}

Furthermore, the compression grid iterates over all regularization strengths and padding values for a specific compression. Since the heatmap is two-dimensional, only the heatmap corresponding to the resulting chosen compression value is displayed. This means first defining the compression range, and nesting the grids inside of this compression, but before the regularization and padding ranges are defined.

\begin{minted}{python}
    # generate all possibilities:
    for k, compress in enumerate(self.compression_range):

        error_grid = np.full((len(self.padding_range), len(self.reg_exponents) * 
            len(self.reg_coeff)), np.nan)
        bounds_grid = np.full((len(self.padding_range), len(self.reg_exponents) * 
            len(self.reg_coeff)), np.nan)
        
        for i, padding in enumerate(self.padding_range):
            reg_index = 0
            for j, reg in enumerate(self.reg_exponents):
                for l, coeff in enumerate(self.reg_coeff):
\end{minted}

In the example run, the following range values are used for the padding, regularization, and compression:

\begin{center}
\begin{tabular}{ | c | c | } 
  \hline
   Padding Range & 6 $\rightarrow$ 61 \\ 
  \hline
  Regularization Coefficient & 1 $\rightarrow$ 8 \\ 
  \hline
  Regularization Exponent & -6 $\rightarrow$ 0 \\ 
  \hline
  Compression Range & 1 $\rightarrow$ 2 \\ 
  \hline
\end{tabular}
\end{center}

With these ranges, the total combinations are:

\begin{center}
    $54 \cdot 6 \cdot 7 \cdot 19 = 44,688$ combinations total
\end{center}

Increasing the number of combinations also increases the processing time. A size of 44,688 combinations takes approximately 1.5 seconds to run, although this can vary for different hardware. For some large combination values, over 1 minute of computation time may be required.
\\
The central loop consists of three nested loops, starting with the regularization, inside padding, inside compression.

\begin{minted}{python}
    for k, compress in enumerate(self.compression_range):
            for i, padding in enumerate(self.padding_range):
                for j, reg in enumerate(self.reg_exponents):
\end{minted}

For each loop iteration, the code runs through a possibility for the precompensated waveform, changing the compression (k), padding (i) and regularization (j) values on each run, until all 44,688 cases are exhausted.
\\
As mentioned previously, the regularization exponent $n$ corresponds to the full regularization strength $1e^{-n}$. Because of this, the full regularization strength, which is the value used in precompensation mathematics, is defined.

\begin{minted}{python}
    reg_strength = 1 ** reg
\end{minted}

Then, the code trials the current iteration case:

\begin{minted}{python}
    dense_time = np.linspace(0, time_array[-1], num_samples)
    dense_waveform = np.interp(dense_time, time_array / compress, waveform)
    original = np.pad(dense_waveform, pad_width=padding, mode='edge')
    time_sliced = dense_time[:len(original)]
    dt = time_sliced[1] - time_sliced[0]
    time = np.arange(len(original)) * dt
    first_index = original[0]
    last_index = float(original[-1])
    
    precompensated = FilterSimulation().precompensation(original, last_index, 
        reg_strength, padding)
    precompensated = precompensated[:len(original)]
    precompensated = FilterSimulation().trimming(precompensated, padding, 
        first_index, last_index)
    precompensated_filtered = FilterSimulation().convolution(precompensated)
    shift_p = np.arange(len(precompensated_filtered)) * dt
    precompensated_filtered = FilterSimulation().interpolation(time, shift_p, 
        precompensated_filtered)
\end{minted}

This code is mentioned more in detail in the previous section on the filter simulation class, and will not be explained here.
\\
The next part of the method is interpreting the results. At the end of the iteration, the maximum absolute error between the original and precompensated filtered waveforms is calculated. The error is also saved as an index [i, j] in the error grid for the heat map later on.

\begin{minted}{python}
    error = np.max(np.abs(original - precompensated_filtered))
    error_grid[i, j] = error
\end{minted}

Then, the penalty calculator comes in to play. As mentioned previously, the penalty is the sum of the weighted values, as given.

\begin{center}
    penalty = $\alpha \; \cdot$ error + $\beta \; \cdot$ padding + $\gamma \; \cdot$ compression
\end{center}

This is given by:

\begin{minted}{python}
    penalty = self.alpha * error + self.beta * padding - self.gamma * compress
\end{minted}

Now, four of the five stored values are filled. Lastly, the iteration needs to be checked to see if it is outside the voltage bounds.

\begin{minted}{python}
    bounds = 1 if np.max(precompensated) > self.upper_voltage_bound or 
        np.min(precompensated) < self.lower_voltage_bound else 0
    bounds_grid[i, j] = bounds
\end{minted}

If the maximum precompensated waveform (y-axis) value is over 40 volts or under -40 volts, the bound tally becomes 1, and will be further disregarded. Otherwise, the iteration is still valid.

Lastly, per iteration, the value of best parameters is constructed.
\begin{minted}{python}
    best_params = (compress, padding, reg_strength, penalty, bounds)
    all_best_params.append(best_params)
    best_error_grid = error_grid.copy()
\end{minted}

\subsubsection{Heatmap Method}
\subsubsection{Error Plotter Method}

\section{Use Cases}
\subsection{Neighboring Segment Transport}
\subsubsection{Example 1}
\subsubsection{Example 2}
\subsection{Multiple Segment Transport}

\section{Analyzing Errors}
\subsection{Hardware Limitations}






\section{Example Case}
\subsection{Input Data}
In this example usage of the algorithm, the input data values are as follows:

\begin{center}
\begin{tabular}{ | c | c | } 
  \hline
   Input Waveform Array & [-1, -0.9990133642, ..., 0] + 1 \\ 
  \hline
  Input Time Array & [0, 0.02, ..., 1] \\ 
  \hline
  Filter Kernel Coefficients & [4.65175217e-05, ..., 1.00004649e+00] \\ 
  \hline
  Minimum DAQ Step & 380 ns \\ 
  \hline
  Total Duration & 19.38 $\mu$s \\ 
  \hline
  Waveform Size & 6 volts \\ 
  \hline
  Minimum Voltage Size & -40 volts \\ 
  \hline
  Maximum Voltage Size & +40 volts \\ 
  \hline
\end{tabular}
\end{center}

\subsection{Optimization Parameters}
Optimization values are further bounded to minimize the runtime of the algorithm:

\begin{center}
\begin{tabular}{ | c | c | } 
  \hline
   Padding Range & 20 $\rightarrow$ 101 \\ 
  \hline
  Regularization (EXP) Range & -5 $\rightarrow$ 0 \\ 
  \hline
  Compression Range & 20 $\rightarrow$ 50 \\ 
  \hline
\end{tabular}
\end{center}

\subsection{Tradeoff Threshold}
The goal of optimization is:

\begin{itemize}
    \item Minimize Signal Length
    \begin{itemize}
        \item Reduce Padding
        \item Increase Compression
    \end{itemize}
    \item Minimize Error (in RMSE)
    \item Find Best Regularization
    \item Keep within Min/Max Voltage Bounds
\end{itemize}

Thus, there is a tradeoff between minimizing error, maximizing the compression, and minimizing the padding.

In the current setup, these are the weights of each parameter:

\begin{center}
\begin{tabular}{ | c | c | } 
  \hline
   Error & 20 1.00 \\ 
  \hline
  Padding & 0.01 \\ 
  \hline
  Compression & 0.01 \\ 
  \hline
\end{tabular}
\end{center}

If a smaller padding/error or larger compression value is favored, the penalty strengths of those can be adjusted accordingly.
\newpage
\subsection{Results}
\subsubsection{Heatmap}
Below is a heatmap of different error rates according to padding size and regularization strength for the best selected compression value. 

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/heatmap1.png}
    \caption{The best compression value here is a compression of 49. The padding range is between 20 and 100, and the regularization strength is between 1e-5 and 0, as previously mentioned. The blue areas correspond to less error (rmse), and the red areas have the most error. The grayed-out areas correspond to values outside the upper and lower voltage bounds of the function and cannot be accessed. The illuminated rectangle is the chosen "best case" of the general heatmap.}
\end{figure} \label{heatmap1}

In the heatmap of potential values, it is clear to see where the least error prone values lie. The goal is to get something with the least padding but deepest blue coloring. The selected tile (highlighted in gold) is at (2, 69), meaning it has a regularization factor of 1e-2 and padding of 69.

The details of this run are explored below:

\begin{minted}{text}
The best padding is: 69
The best regularization is: 0.001
Root Mean Squared Error (RMSE): 0.73543 V
Max Absolute Error: 3.07936 V
\end{minted}

This result is printed from the function, and confirms the best case is selected, as shown on the heatmap.
\newpage
\subsubsection{Error in Context}
Now that the input has undergone optimization, the error and precompensated waveform can be analyzed. Below is a depiction of the average rmse of the function at different points in time throughout the length of the entire signal.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/error1.png}
    \caption{A plot of error (rmse) over time for the full length of the signal. On the y-axis is the absolute error magnitude at a given point in time of the signal, which is represented on the x-axis. The peak of the error, also printed in the last section, is the maximum absolute error of the function.}
\end{figure} \label{error1}

The Max Absolute Error value, which is the point of greatest error in the entire signal is just over 3 volts, which matches perfectly with the printed Max Absolute Error value of 3.07936 volts (from the last section).
\newpage
\subsubsection{Function Plot 1}
Finally, the plot of the actual precompensated waveform in comparison to the original is plotted below.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/graph1.png}
    \caption{There are four functions plotted above. The first is the original (blue), unchanged function. The second (orange) is the filtered (convoluted) waveform of this original, showing the filtering effects on the input signal. Third is the precompensated but unfiltered waveform (green), and fourth is the precompensated and filtered one (red). The goal is to match the red function as closely as possible to the blue one, as we are trying to estimate the original.}
\end{figure} \label{graph1}

The best-case error graph matches nicely with this function graph, as the time points align in terms of error behavior. The areas of greatest deviance correspond to the biggest error, and are the subject of interest and focus.

If this deviation between red and blue is too significant, it can be reduced at the expense of a larger signal. This can be done by adjusting the threshold tradeoff parameters.
\newpage
\subsubsection{Function Plot 2}
The second plot shows an example of adjusting the threshold parameters to favoring smaller rmse values and prioritizing padding less. It is further helpful to increase the padding index, to allow for an even better margin of error.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/graph2.png}
    \caption{Reevaluated function at lower error values but longer time scales.}
\end{figure} \label{graph2}

Compared to the first function plot, the second one is much closer to aligning with the original (blue) waveform, but at the expense of a longer waveform. These are the bounds and limits of the precompensation.
\newpage
As for the exact error plots of this new function, the error margin is smaller and has a much lower maximum than function plot 1.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/error2.png}
    \caption{Reevaluated error function at lower error values but longer time scales.}
\end{figure} \label{error2}

The printed output matches this graph:

\begin{minted}{text}
The best padding is: 299
The best regularization is: 0.001
Root Mean Squared Error (RMSE): 0.34712 V
Max Absolute Error: 2.74936 V
\end{minted}

\subsubsection{Comparison Analysis}
Here, the signal only has a max absolute error of 2.74936 volts, as opposed to the first function's 3.07936 volts. This is a 0.33-volt difference, which may not seem very much, but has a significant effect on the waveform (as seen in the plot). The tradeoff here is that the function is longer, from 1.4 $\mu$s to now 12 $\mu$s.
\subsubsection{Hardware Limitations}
This is the best we can do with time steps of 380 ns, which sets a maximum on our resolution value. For a waveform of 17.5 $\mu$s, which is the uncompressed duration of the above two plots, there are only

\begin{equation}
    \dfrac{17.5 \mu s}{380 ns} = \dfrac{17.5 \mu s}{0.38 \mu s} \approx 46
\end{equation}
46 samples in total. 

Because of this limit of 380 ns or higher resolution (more samples), the waveform would need to be longer in order to have a higher accuracy, which is the opposite of what compression does, and what is desired. To increase the resolution and decrease the waveform size while still retaining accuracy, a DAQ below a sampling rate corresponding to 380 ns would need to be used. 

Another limitation of the waveform precompensation is the voltage bound of $\pm 40 $ volts. In the heatmaps, the "grayed" area represents values that go beyond this threshold. Some of these values hold low error-low padding values, which could be better case scenarios than the selected case. For these to be accessible, the hardware used would need to have values above and below 40 volts.

\section{Additional Case -- Non Neighboring Segments}
For voltage ramps across multiple segments, a different waveform is used. This waveform also works best with adjusted optimal parameters:

\begin{center}
\begin{tabular}{ | c | c | } 
  \hline
   Padding Range & 80 $\rightarrow$ 200 \\ 
  \hline
  Regularization (EXP) Range & -5 $\rightarrow$ 0 \\ 
  \hline
  Compression Range & 1 $\rightarrow$ 2 \\ 
  \hline
\end{tabular}
\end{center}

For this example, the threshold is also optimized for smaller errors, but can be adjusted as needed. Then the output function is given below.

\begin{figure}[!h]
  \centering
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{Figures/heatmap3.png}
    \caption{Heatmap of the graph.}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{Figures/error3.png}
    \caption{Error per time of the graph.}
  \end{minipage}
\end{figure}

As emphasized more clearly in this case, having voltage bounds can pose a large limitation on the best-case value of the precompensated waveform. The optimized function is given below:

\begin{figure}[h!]
    \centering
\includegraphics[width=0.8\linewidth]{Figures/graph3.png}
    \caption{Voltage waveform for non-neighboring segments, optimized for accuracy.}
\end{figure} \label{graph3}

Lastly, to better demonstrate the tradeoff between values, another plot is shown, optimized for different parameters:

\begin{figure}[h!]
    \centering
\includegraphics[width=0.8\linewidth]{Figures/graph4.png}
    \caption{Voltage waveform for non-neighboring segments, optimized for accuracy.}
\end{figure} \label{graph4}

As a closing note, the heatmap for this graph seems to have a very good potential best-case value at (0, 61), also known as 1e-0 regularization and 61 units of padding. Unfortunately, this value is out of bounds for our function and cannot be used.

\begin{figure}[h!]
    \centering
\includegraphics[width=1\linewidth]{Figures/heatmap4.png}
    \caption{Voltage waveform for non-neighboring segments, optimized for accuracy.}
\end{figure} \label{heatmap4}

In future iterations with different hardware values, there is a large margin for improvement of the precompensation.
\section{Benchmarking}
\section{Code Reference}
For reference to my direct algorithm code used to build this system, you can \href{https://github.com/OnekaSingh/Qiskit-Tutorial-Projects/blob/main/transport_ramps.ipynb}{click here}, or visit the following link:
\begin{minted}{text}
https://github.com/OnekaSingh/Qiskit-Tutorial-Projects/blob/main/transport_ramps.ipynb
\end{minted}
\end{document}
